O [[Spark]] surgiu como um projeto de pesquisa - 2009
"Spark: [[Cluster]] Computing With Working Sets"
	Matei Zahara

```ad-info
title: Execução

O Spark pode ser executado sozinho ou em vários gerenciadores de Cluster existentes. Atualmente oferece várias opções de implantação

Sua capacidade de processamento em memoria utilizando caxamento em memória que otimiza 

Ele lê dados do custer que cada etaparequer uma leitura
```

Processamento de dados estruturados e não estruturados

ELE SURGE APARTIR DE DA IDEA DA Hadoop - CRIADO APARTID DE AMBIENTES DISTRIBUIDOS 
esse drive consegue fazer a distribuição 
Hadoop ele faz todas as operações de escrita no hd - lendo e escrevendo
Spark (opensource) na memória RAM todos os passos que o Hadoop faz ele faz lendo a memória RAM comportamento laser (PREGUIÇOSO)

tranformations ela não faz nada lea programa a execução 
action - ele vai fazer a cadeia que programou 

tranformation ele analisa o meta dados - planejamento